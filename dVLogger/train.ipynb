{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LOWER_LIMIT = -7 # np.exp(-7) = 0.00091188196555451624\n",
    "SAVE_EVALUATIONS = True\n",
    "# SAVE_FEATURES = False\n",
    "# SAVE_OUTPUTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_name = platform.system()\n",
    "if system_name == 'Linux':\n",
    "    github_dir = '/home/gor/codes/dVLogger-Project'\n",
    "elif system_name == 'Darwin':\n",
    "    github_dir = '/Users/gor/codes/dVLogger-Project'\n",
    "features_npz = os.path.join(github_dir, 'data', 'case_sum_0706_A_1.npz')\n",
    "clinical_outcomes_npz = os.path.join(github_dir, 'data', 'clinical_outcomes_0707.npz')\n",
    "folds_clinical_outcomes_npz = os.path.join(github_dir, 'data', 'folds_clinical_outcomes_0707.npz')\n",
    "\n",
    "d = datetime.date.today()\n",
    "os.makedirs(os.path.join(github_dir, 'results', '{:02d}{:02d}'.format(d.month, d.day)), exist_ok=True)\n",
    "clf_eval_csv = os.path.join(github_dir,\n",
    "                            'results',\n",
    "                            '{:02d}{:02d}'.format(d.month, d.day),\n",
    "                            'clinical_outcomes_classification_results.csv')\n",
    "reg_eval_csv = os.path.join(github_dir,\n",
    "                            'results',\n",
    "                            '{:02d}{:02d}'.format(d.month, d.day),\n",
    "                            'clinical_outcomes_regression_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean=None, std=None):\n",
    "    if mean is None:\n",
    "        mean = x.mean(axis=0)\n",
    "        std = x.std(axis=0)\n",
    "        std[std < 1e-9] = 1e-9\n",
    "    return (x - mean) / std, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_one_hot(y):\n",
    "    assert(y.ndim == 1 or y.shape[1] == 1)\n",
    "    assert(np.isclose(np.min(y), 0))\n",
    "    n_samples = y.shape[0]\n",
    "    class_count = np.int32(np.max(y) + 1)\n",
    "    r = np.zeros((n_samples, class_count), dtype=np.int32)\n",
    "    r[(np.arange(n_samples), y.astype(np.int32))] = 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = [\n",
    "    LogisticRegression('l2'),\n",
    "    SVC(kernel='rbf', decision_function_shape='ovr'),\n",
    "    RandomForestClassifier(n_estimators=20),\n",
    "    RandomForestClassifier(n_estimators=50),\n",
    "]\n",
    "clf_name_list = ['LR-L2', 'SVM-RBF', 'RF-20', 'RF-50']\n",
    "clf_need_norm_list = [False, True, False, False]\n",
    "clf_eval_list = {\n",
    "    'Acc': metrics.accuracy_score,\n",
    "    'Prec': lambda a, b: metrics.precision_score(a, b, average='micro'),\n",
    "    'Rec': lambda a, b: metrics.recall_score(a, b, average='micro'),\n",
    "    'AUROC': lambda a, b: metrics.roc_auc_score(a, b, average='micro'),\n",
    "}\n",
    "reg_list = [\n",
    "    LinearRegression('l1'),\n",
    "    LinearRegression('l2'),\n",
    "    RandomForestRegressor(n_estimators=20),\n",
    "    RandomForestRegressor(n_estimators=50),\n",
    "    # SVR: The free parameters in the model are C and epsilon\n",
    "#     SVR(kernel='linear'),\n",
    "#     SVR(kernel='poly'),\n",
    "#     SVR(kernel='rbf'),\n",
    "]\n",
    "reg_name_list = [\n",
    "    'LiR-L1',\n",
    "    'LiR-L2',\n",
    "    'RF-Reg-20',\n",
    "    'RF-Reg-50',\n",
    "#     'SVR-Li',\n",
    "#     'SVR-Poly',\n",
    "#     'SVR-RBF',\n",
    "]\n",
    "reg_need_norm_list = [False] * len(reg_list)\n",
    "reg_eval_list = {\n",
    "    'MAE': metrics.mean_absolute_error,\n",
    "#     'Explained-Variance-Sc': metrics.explained_variance_score,\n",
    "    'R2-Sc': metrics.r2_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals().update(np.load(features_npz))\n",
    "locals().update(np.load(clinical_outcomes_npz))\n",
    "locals().update(np.load(folds_clinical_outcomes_npz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.all(X_case == Y_case))\n",
    "case = X_case\n",
    "n_splits = folds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_dev_set:\n",
    "    data_set_list = ['train', 'dev', 'test']\n",
    "else:\n",
    "    data_set_list = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_EVALUATIONS:\n",
    "    header = ['y_name', 'model_name']\n",
    "    for p1 in data_set_list:\n",
    "        for p3 in clf_eval_list.keys():\n",
    "            for p2 in ['mean', 'std']:\n",
    "                header.append('_'.join([p1, p2, p3]))\n",
    "    with open(clf_eval_csv, 'w') as f:\n",
    "        f.write(','.join(header) + '\\n')\n",
    "    \n",
    "    header = ['y_name', 'model_name']\n",
    "    for p1 in data_set_list:\n",
    "        for p3 in reg_eval_list.keys():\n",
    "            for p2 in ['mean', 'std']:\n",
    "                header.append('_'.join([p1, p2, p3]))\n",
    "    with open(reg_eval_csv, 'w') as f:\n",
    "        f.write(','.join(header) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_idx in range(len(Y_col)):\n",
    "    y = Y[:, y_idx]\n",
    "    y_name = Y_col[y_idx]\n",
    "    y_train_task_type = Y_train_task_type[y_idx]\n",
    "    print(y_name)\n",
    "\n",
    "    if y_train_task_type == 'classification':\n",
    "        model_list = clf_list\n",
    "        model_name_list = clf_name_list\n",
    "        model_need_norm_list = clf_need_norm_list\n",
    "        model_eval_list = clf_eval_list\n",
    "    elif y_train_task_type == 'regression':\n",
    "        model_list = reg_list\n",
    "        model_name_list = reg_name_list\n",
    "        model_need_norm_list = reg_need_norm_list\n",
    "        model_eval_list = reg_eval_list\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    for model_idx in range(len(model_list)):\n",
    "        model = model_list[model_idx]\n",
    "        model_name = model_name_list[model_idx]\n",
    "        model_need_norm = model_need_norm_list[model_idx]\n",
    "        print('{} : '.format(model_name), end='')\n",
    "\n",
    "        eval_train, eval_test = {}, {}\n",
    "        for eval_name in model_eval_list:\n",
    "            eval_train[eval_name] = []\n",
    "            eval_test[eval_name] = []\n",
    "        if has_dev_set:\n",
    "            eval_dev = {}\n",
    "            for eval_name in model_eval_list:\n",
    "                eval_dev[eval_name] = []\n",
    "\n",
    "        for split_idx in range(n_splits):\n",
    "            train_idx = folds[y_idx, split_idx, 0]\n",
    "            if has_dev_set:\n",
    "                dev_idx = folds[y_idx, split_idx, 1]\n",
    "                test_idx = folds[y_idx, split_idx, 2]\n",
    "            else:\n",
    "                test_idx = folds[y_idx, split_idx, 1]\n",
    "            if model_need_norm:\n",
    "                x_train, x_mean, x_std = normalize(X[train_idx, :])\n",
    "                if has_dev_set:\n",
    "                    x_dev, _, _ = normalize(X[dev_idx, :], x_mean, x_std)\n",
    "                x_test, _, _ = normalize(X[test_idx, :], x_mean, x_std)\n",
    "            else:\n",
    "                x_train = X[train_idx, :]\n",
    "                if has_dev_set:\n",
    "                    x_dev = X[dev_idx, :]\n",
    "                x_test = X[test_idx, :]\n",
    "            y_train = Y[train_idx, y_idx]\n",
    "            if has_dev_set:\n",
    "                y_dev = Y[dev_idx, y_idx]\n",
    "            y_test = Y[test_idx, y_idx]\n",
    "\n",
    "            print('{}'.format(split_idx + 1), end='')\n",
    "            model.fit(x_train, y_train)\n",
    "            y_pred_train = model.predict(x_train)\n",
    "            if has_dev_set:\n",
    "                y_pred_dev = model.predict(x_dev)\n",
    "            y_pred_test = model.predict(x_test)\n",
    "            if y_train_task_type == 'classification':\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    y_score_train = model.predict_proba(x_train)\n",
    "                    if has_dev_set:\n",
    "                        y_score_dev = model.predict_proba(x_dev)\n",
    "                    y_score_test = model.predict_proba(x_test)\n",
    "                else:\n",
    "                    y_score_train = model.decision_function(x_train)\n",
    "                    if has_dev_set:\n",
    "                        y_score_dev = model.decision_function(x_dev)\n",
    "                    y_score_test = model.decision_function(x_test)\n",
    "\n",
    "            if '_log_' in y_name:\n",
    "                y_train = np.exp(y_train)\n",
    "                y_pred_train[y_pred_train < LOG_LOWER_LIMIT] = LOG_LOWER_LIMIT\n",
    "                y_pred_train = np.exp(y_pred_train)\n",
    "                if has_dev_set:\n",
    "                    y_dev = np.exp(y_dev)\n",
    "                    y_pred_dev[y_pred_dev < LOG_LOWER_LIMIT] = LOG_LOWER_LIMIT\n",
    "                    y_pred_dev = np.exp(y_pred_dev)\n",
    "                y_test = np.exp(y_test)\n",
    "                y_pred_test[y_pred_test < LOG_LOWER_LIMIT] = LOG_LOWER_LIMIT\n",
    "                y_pred_test = np.exp(y_pred_test)\n",
    "            \n",
    "            print(', ', end='')\n",
    "            for eval_name, eval_fn in model_eval_list.items():\n",
    "                if eval_name == 'AUROC':\n",
    "                    if y_score_train.ndim == 1:\n",
    "                        eval_train[eval_name].append(eval_fn(y_train, y_score_train))\n",
    "                        if has_dev_set:\n",
    "                            eval_dev[eval_name].append(eval_fn(y_dev, y_score_dev))\n",
    "                        eval_test[eval_name].append(eval_fn(y_test, y_score_test))\n",
    "                    else:\n",
    "                        eval_train[eval_name].append(eval_fn(as_one_hot(y_train), y_score_train))\n",
    "                        if has_dev_set:\n",
    "                            eval_dev[eval_name].append(eval_fn(as_one_hot(y_dev), y_score_dev))\n",
    "                        eval_test[eval_name].append(eval_fn(as_one_hot(y_test), y_score_test))\n",
    "                else:\n",
    "                    eval_train[eval_name].append(eval_fn(y_train, y_pred_train))\n",
    "                    if has_dev_set:\n",
    "                        eval_dev[eval_name].append(eval_fn(y_dev, y_pred_dev))\n",
    "                    eval_test[eval_name].append(eval_fn(y_test, y_pred_test))\n",
    "        print()\n",
    "\n",
    "        for eval_name in model_eval_list:\n",
    "            if has_dev_set:\n",
    "                print('{:21s} :: Trn: {:.3f} ({:.3f}), Dev: {:.3f} ({:.3f}), Tst: {:.3f} ({:.3f})'.format(\n",
    "                    eval_name,\n",
    "                    np.mean(eval_train[eval_name]), np.std(eval_train[eval_name]),\n",
    "                    np.mean(eval_dev[eval_name]), np.std(eval_dev[eval_name]),\n",
    "                    np.mean(eval_test[eval_name]), np.std(eval_test[eval_name])))\n",
    "            else:\n",
    "                print('{:21s} :: Trn: {:.3f} ({:.3f}), Tst: {:.3f} ({:.3f})'.format(\n",
    "                    eval_name,\n",
    "                    np.mean(eval_train[eval_name]), np.std(eval_train[eval_name]),\n",
    "                    np.mean(eval_test[eval_name]), np.std(eval_test[eval_name])))\n",
    "            \n",
    "        if SAVE_EVALUATIONS:\n",
    "#             header = ['y_name', 'model_name']\n",
    "#             for p1 in ['train', 'dev', 'test']:\n",
    "#                 for p3 in reg_eval_list.keys():\n",
    "#                     for p2 in ['mean', 'std']:\n",
    "#                         header.append('_'.join([p1, p2, p3]))\n",
    "            row = [y_name, model_name]\n",
    "            for p1 in data_set_list:\n",
    "                for p3 in model_eval_list.keys():\n",
    "                    for p2 in ['mean', 'std']:\n",
    "                        row.append(eval('\"{:.3f}\".format(np.' + p2 + '(eval_' + p1 + '[\"' + p3 + '\"]))'))\n",
    "            if y_train_task_type == 'classification':\n",
    "                with open(clf_eval_csv, 'a') as f:\n",
    "                    f.write(','.join(row) + '\\n')\n",
    "            elif y_train_task_type == 'regression':\n",
    "                with open(reg_eval_csv, 'a') as f:\n",
    "                    f.write(','.join(row) + '\\n')\n",
    "            else:\n",
    "                raise ValueError\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
